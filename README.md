Recent work has shown that small, iterative models such as Tiny Recursive Models can outperform large language models on complex reasoning problems such as those in the Abstraction and Reasoning Corpus (ARC). In our paper, we ask two simple questions: why are such models so efficient on the ARC-AGI problem, and how can they be improved? To answer these questions, we consider TRMs as implicit policy improvement algorithm. Based on this, we propose a new learning method that provides target for each model loop during the training process. In practice, we demonstrate that our approach makes training much more efficient. We avoid learnable halting mechanisms and simplify latent reasoning steps, reducing the total number of forward passes by 24x. Notably, we achieved 23$\%$ accuracy on the ARC-1 problem with just 0.8 million parameters, outperforming most large language models, and achieved the same quality as TRM with fewer resources.
