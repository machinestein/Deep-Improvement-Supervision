Recent work shows compact models using looped, iterative refinement (e.g., Hierarchical Reasoning Model and Tiny Recursive Model) can outperform large language models on complex reasoning problems like Abstraction and Reasoning Corpus (ARC-AGI). We introduce a new learning algorithm that provides supervision for this looped reasoning process. Our method enables a significantly simpler architecture variant and fewer parameters to achieve state-of-the-art performance on both ARC-AGI 1, ARC-AGI 2 without any external data.
